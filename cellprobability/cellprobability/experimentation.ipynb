{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rz200\\AppData\\Local\\anaconda3\\envs\\cellprob\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports from project\n",
    "from import_images import import_images_from_path\n",
    "from cellpose_data import get_cellpose_probability_maps\n",
    "from random_crops import get_random_crops_from_multiple_images\n",
    "from augment_data import rotate_images_and_cellprobs_return_merged\n",
    "from u_net import UNet\n",
    "\n",
    "#Import from other libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File directories\n",
    "images_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing images\n",
    "images = import_images_from_path(images_path,num_imgs=23,normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting groundtruth data from images with Cellpose\n",
    "cell_probabilities = get_cellpose_probability_maps(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the cell_probabilities to be between 0 and 1\n",
    "cell_probabilities_norm = [(cellprob-np.min(cellprob))/(np.max(cellprob)-np.min(cellprob)) for cellprob in cell_probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting tensor crops of 128x128 from images\n",
    "image_crops, cellprob_crops = get_random_crops_from_multiple_images(images,cell_probabilities_norm,num_crops=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmenting the data\n",
    "image_crops_augmented, cellprob_crops_augmented = rotate_images_and_cellprobs_return_merged(image_crops,cellprob_crops,angles=[90,180,270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_crops_augmented, cellprob_crops_augmented, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300 | Train Loss: 0.0349 | Test Loss: 0.0250\n",
      "Epoch: 2/300 | Train Loss: 0.4538 | Test Loss: 0.0258\n",
      "Epoch: 3/300 | Train Loss: 0.0251 | Test Loss: 0.0248\n",
      "Epoch: 4/300 | Train Loss: 0.0250 | Test Loss: 0.0240\n",
      "Epoch: 5/300 | Train Loss: 0.0278 | Test Loss: 0.0286\n",
      "Epoch: 6/300 | Train Loss: 0.0243 | Test Loss: 0.0391\n",
      "Epoch: 7/300 | Train Loss: 0.0241 | Test Loss: 0.0234\n",
      "Epoch: 8/300 | Train Loss: 0.0233 | Test Loss: 0.0231\n",
      "Epoch: 9/300 | Train Loss: 0.0238 | Test Loss: 0.0237\n",
      "Epoch: 10/300 | Train Loss: 0.0229 | Test Loss: 0.0224\n",
      "Epoch: 11/300 | Train Loss: 0.0225 | Test Loss: 0.0258\n",
      "Epoch: 12/300 | Train Loss: 0.0222 | Test Loss: 0.0222\n",
      "Epoch: 13/300 | Train Loss: 0.0218 | Test Loss: 0.0226\n",
      "Epoch: 14/300 | Train Loss: 0.0216 | Test Loss: 0.0219\n",
      "Epoch: 15/300 | Train Loss: 0.0218 | Test Loss: 0.0220\n",
      "Epoch: 16/300 | Train Loss: 0.0213 | Test Loss: 0.0218\n",
      "Epoch: 17/300 | Train Loss: 0.0216 | Test Loss: 0.0227\n",
      "Epoch: 18/300 | Train Loss: 0.0214 | Test Loss: 0.0216\n",
      "Epoch: 19/300 | Train Loss: 0.0215 | Test Loss: 0.0219\n",
      "Epoch: 20/300 | Train Loss: 0.0212 | Test Loss: 0.0221\n",
      "Epoch: 21/300 | Train Loss: 0.0212 | Test Loss: 0.0219\n",
      "Epoch: 22/300 | Train Loss: 0.0213 | Test Loss: 0.0215\n",
      "Epoch: 23/300 | Train Loss: 0.0217 | Test Loss: 0.0215\n",
      "Epoch: 24/300 | Train Loss: 0.0213 | Test Loss: 0.0222\n",
      "Epoch: 25/300 | Train Loss: 0.0212 | Test Loss: 0.0217\n",
      "Epoch: 26/300 | Train Loss: 0.0213 | Test Loss: 0.0215\n",
      "Epoch: 27/300 | Train Loss: 0.0215 | Test Loss: 0.0215\n",
      "Epoch: 28/300 | Train Loss: 0.0213 | Test Loss: 0.0214\n",
      "Epoch: 29/300 | Train Loss: 0.0213 | Test Loss: 0.0220\n",
      "Epoch: 30/300 | Train Loss: 0.0214 | Test Loss: 0.0215\n",
      "Epoch: 31/300 | Train Loss: 0.0213 | Test Loss: 0.0221\n",
      "Epoch: 32/300 | Train Loss: 0.0212 | Test Loss: 0.0213\n",
      "Epoch: 33/300 | Train Loss: 0.0210 | Test Loss: 0.0222\n",
      "Epoch: 34/300 | Train Loss: 0.0211 | Test Loss: 0.0219\n",
      "Epoch: 35/300 | Train Loss: 0.0211 | Test Loss: 0.0215\n",
      "Epoch: 36/300 | Train Loss: 0.0213 | Test Loss: 0.0218\n",
      "Epoch: 37/300 | Train Loss: 0.0212 | Test Loss: 0.0215\n",
      "Epoch: 38/300 | Train Loss: 0.0210 | Test Loss: 0.0213\n",
      "Epoch: 39/300 | Train Loss: 0.0210 | Test Loss: 0.0221\n",
      "Epoch: 40/300 | Train Loss: 0.0211 | Test Loss: 0.0213\n",
      "Epoch: 41/300 | Train Loss: 0.0210 | Test Loss: 0.0218\n",
      "Epoch: 42/300 | Train Loss: 0.0211 | Test Loss: 0.0215\n",
      "Epoch: 43/300 | Train Loss: 0.0210 | Test Loss: 0.0214\n",
      "Epoch: 44/300 | Train Loss: 0.0216 | Test Loss: 0.0221\n",
      "Epoch: 45/300 | Train Loss: 0.0210 | Test Loss: 0.0218\n",
      "Epoch: 46/300 | Train Loss: 0.0212 | Test Loss: 0.0216\n",
      "Epoch: 47/300 | Train Loss: 0.0211 | Test Loss: 0.0214\n",
      "Epoch: 48/300 | Train Loss: 0.0210 | Test Loss: 0.0213\n",
      "Epoch: 49/300 | Train Loss: 0.0212 | Test Loss: 0.0221\n",
      "Epoch: 50/300 | Train Loss: 0.0212 | Test Loss: 0.0210\n",
      "Epoch: 51/300 | Train Loss: 0.0209 | Test Loss: 0.0215\n",
      "Epoch: 52/300 | Train Loss: 0.0209 | Test Loss: 0.0214\n",
      "Epoch: 53/300 | Train Loss: 0.0209 | Test Loss: 0.0221\n",
      "Epoch: 54/300 | Train Loss: 0.0213 | Test Loss: 0.0216\n",
      "Epoch: 55/300 | Train Loss: 0.0208 | Test Loss: 0.0216\n",
      "Epoch: 56/300 | Train Loss: 0.0211 | Test Loss: 0.0216\n",
      "Epoch: 57/300 | Train Loss: 0.0209 | Test Loss: 0.0215\n",
      "Epoch: 58/300 | Train Loss: 0.0219 | Test Loss: 0.0217\n",
      "Epoch: 59/300 | Train Loss: 0.0209 | Test Loss: 0.0212\n",
      "Epoch: 60/300 | Train Loss: 0.0212 | Test Loss: 0.0214\n",
      "Epoch: 61/300 | Train Loss: 0.0209 | Test Loss: 0.0213\n",
      "Epoch: 62/300 | Train Loss: 0.0210 | Test Loss: 0.0213\n",
      "Epoch: 63/300 | Train Loss: 0.0210 | Test Loss: 0.0216\n",
      "Epoch: 64/300 | Train Loss: 0.0209 | Test Loss: 0.0212\n",
      "Epoch: 65/300 | Train Loss: 0.0210 | Test Loss: 0.0213\n",
      "Epoch: 66/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 67/300 | Train Loss: 0.0209 | Test Loss: 0.0210\n",
      "Epoch: 68/300 | Train Loss: 0.0209 | Test Loss: 0.0212\n",
      "Epoch: 69/300 | Train Loss: 0.0209 | Test Loss: 0.0215\n",
      "Epoch: 70/300 | Train Loss: 0.0208 | Test Loss: 0.0211\n",
      "Epoch: 71/300 | Train Loss: 0.0210 | Test Loss: 0.0214\n",
      "Epoch: 72/300 | Train Loss: 0.0208 | Test Loss: 0.0215\n",
      "Epoch: 73/300 | Train Loss: 0.0208 | Test Loss: 0.0215\n",
      "Epoch: 74/300 | Train Loss: 0.0210 | Test Loss: 0.0213\n",
      "Epoch: 75/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 76/300 | Train Loss: 0.0213 | Test Loss: 0.0218\n",
      "Epoch: 77/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 78/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 79/300 | Train Loss: 0.0209 | Test Loss: 0.0213\n",
      "Epoch: 80/300 | Train Loss: 0.0210 | Test Loss: 0.0215\n",
      "Epoch: 81/300 | Train Loss: 0.0208 | Test Loss: 0.0219\n",
      "Epoch: 82/300 | Train Loss: 0.0210 | Test Loss: 0.0217\n",
      "Epoch: 83/300 | Train Loss: 0.0212 | Test Loss: 0.0213\n",
      "Epoch: 84/300 | Train Loss: 0.0210 | Test Loss: 0.0211\n",
      "Epoch: 85/300 | Train Loss: 0.0210 | Test Loss: 0.0212\n",
      "Epoch: 86/300 | Train Loss: 0.0210 | Test Loss: 0.0208\n",
      "Epoch: 87/300 | Train Loss: 0.0209 | Test Loss: 0.0218\n",
      "Epoch: 88/300 | Train Loss: 0.0210 | Test Loss: 0.0229\n",
      "Epoch: 89/300 | Train Loss: 0.0209 | Test Loss: 0.0217\n",
      "Epoch: 90/300 | Train Loss: 0.0209 | Test Loss: 0.0214\n",
      "Epoch: 91/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 92/300 | Train Loss: 0.0209 | Test Loss: 0.0214\n",
      "Epoch: 93/300 | Train Loss: 0.0209 | Test Loss: 0.0216\n",
      "Epoch: 94/300 | Train Loss: 0.0209 | Test Loss: 0.0214\n",
      "Epoch: 95/300 | Train Loss: 0.0209 | Test Loss: 0.0210\n",
      "Epoch: 96/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 97/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 98/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 99/300 | Train Loss: 0.0208 | Test Loss: 0.0214\n",
      "Epoch: 100/300 | Train Loss: 0.0209 | Test Loss: 0.0212\n",
      "Epoch: 101/300 | Train Loss: 0.0207 | Test Loss: 0.0214\n",
      "Epoch: 102/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 103/300 | Train Loss: 0.0209 | Test Loss: 0.0213\n",
      "Epoch: 104/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 105/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 106/300 | Train Loss: 0.0206 | Test Loss: 0.0215\n",
      "Epoch: 107/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 108/300 | Train Loss: 0.0209 | Test Loss: 0.0216\n",
      "Epoch: 109/300 | Train Loss: 0.0210 | Test Loss: 0.0215\n",
      "Epoch: 110/300 | Train Loss: 0.0206 | Test Loss: 0.0215\n",
      "Epoch: 111/300 | Train Loss: 0.0209 | Test Loss: 0.0216\n",
      "Epoch: 112/300 | Train Loss: 0.0211 | Test Loss: 0.0214\n",
      "Epoch: 113/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 114/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 115/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 116/300 | Train Loss: 0.0209 | Test Loss: 0.0215\n",
      "Epoch: 117/300 | Train Loss: 0.0206 | Test Loss: 0.0213\n",
      "Epoch: 118/300 | Train Loss: 0.0207 | Test Loss: 0.0214\n",
      "Epoch: 119/300 | Train Loss: 0.0208 | Test Loss: 0.0214\n",
      "Epoch: 120/300 | Train Loss: 0.0209 | Test Loss: 0.0215\n",
      "Epoch: 121/300 | Train Loss: 0.0207 | Test Loss: 0.0222\n",
      "Epoch: 122/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 123/300 | Train Loss: 0.0207 | Test Loss: 0.0215\n",
      "Epoch: 124/300 | Train Loss: 0.0208 | Test Loss: 0.0210\n",
      "Epoch: 125/300 | Train Loss: 0.0207 | Test Loss: 0.0219\n",
      "Epoch: 126/300 | Train Loss: 0.0208 | Test Loss: 0.0211\n",
      "Epoch: 127/300 | Train Loss: 0.0208 | Test Loss: 0.0210\n",
      "Epoch: 128/300 | Train Loss: 0.0211 | Test Loss: 0.0211\n",
      "Epoch: 129/300 | Train Loss: 0.0208 | Test Loss: 0.0211\n",
      "Epoch: 130/300 | Train Loss: 0.0207 | Test Loss: 0.0218\n",
      "Epoch: 131/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 132/300 | Train Loss: 0.0208 | Test Loss: 0.0217\n",
      "Epoch: 133/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 134/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 135/300 | Train Loss: 0.0208 | Test Loss: 0.0216\n",
      "Epoch: 136/300 | Train Loss: 0.0206 | Test Loss: 0.0218\n",
      "Epoch: 137/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 138/300 | Train Loss: 0.0208 | Test Loss: 0.0217\n",
      "Epoch: 139/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 140/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 141/300 | Train Loss: 0.0208 | Test Loss: 0.0214\n",
      "Epoch: 142/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 143/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 144/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 145/300 | Train Loss: 0.0207 | Test Loss: 0.0215\n",
      "Epoch: 146/300 | Train Loss: 0.0212 | Test Loss: 0.0213\n",
      "Epoch: 147/300 | Train Loss: 0.0206 | Test Loss: 0.0215\n",
      "Epoch: 148/300 | Train Loss: 0.0209 | Test Loss: 0.0212\n",
      "Epoch: 149/300 | Train Loss: 0.0206 | Test Loss: 0.0216\n",
      "Epoch: 150/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 151/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 152/300 | Train Loss: 0.0207 | Test Loss: 0.0214\n",
      "Epoch: 153/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 154/300 | Train Loss: 0.0208 | Test Loss: 0.0214\n",
      "Epoch: 155/300 | Train Loss: 0.0206 | Test Loss: 0.0214\n",
      "Epoch: 156/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 157/300 | Train Loss: 0.0210 | Test Loss: 0.0237\n",
      "Epoch: 158/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 159/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 160/300 | Train Loss: 0.0207 | Test Loss: 0.0216\n",
      "Epoch: 161/300 | Train Loss: 0.0207 | Test Loss: 0.0210\n",
      "Epoch: 162/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 163/300 | Train Loss: 0.0214 | Test Loss: 0.0213\n",
      "Epoch: 164/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 165/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 166/300 | Train Loss: 0.0208 | Test Loss: 0.0212\n",
      "Epoch: 167/300 | Train Loss: 0.0206 | Test Loss: 0.0214\n",
      "Epoch: 168/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 169/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 170/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 171/300 | Train Loss: 0.0205 | Test Loss: 0.0210\n",
      "Epoch: 172/300 | Train Loss: 0.0205 | Test Loss: 0.0209\n",
      "Epoch: 173/300 | Train Loss: 0.0206 | Test Loss: 0.0213\n",
      "Epoch: 174/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 175/300 | Train Loss: 0.0206 | Test Loss: 0.0217\n",
      "Epoch: 176/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 177/300 | Train Loss: 0.0207 | Test Loss: 0.0215\n",
      "Epoch: 178/300 | Train Loss: 0.0205 | Test Loss: 0.0220\n",
      "Epoch: 179/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 180/300 | Train Loss: 0.0207 | Test Loss: 0.0214\n",
      "Epoch: 181/300 | Train Loss: 0.0214 | Test Loss: 0.0213\n",
      "Epoch: 182/300 | Train Loss: 0.0207 | Test Loss: 0.0215\n",
      "Epoch: 183/300 | Train Loss: 0.0209 | Test Loss: 0.0214\n",
      "Epoch: 184/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 185/300 | Train Loss: 0.0209 | Test Loss: 0.0217\n",
      "Epoch: 186/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 187/300 | Train Loss: 0.0205 | Test Loss: 0.0210\n",
      "Epoch: 188/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 189/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 190/300 | Train Loss: 0.0207 | Test Loss: 0.0218\n",
      "Epoch: 191/300 | Train Loss: 0.0206 | Test Loss: 0.0216\n",
      "Epoch: 192/300 | Train Loss: 0.0206 | Test Loss: 0.0210\n",
      "Epoch: 193/300 | Train Loss: 0.0204 | Test Loss: 0.0214\n",
      "Epoch: 194/300 | Train Loss: 0.0207 | Test Loss: 0.0214\n",
      "Epoch: 195/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 196/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 197/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 198/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 199/300 | Train Loss: 0.0206 | Test Loss: 0.0210\n",
      "Epoch: 200/300 | Train Loss: 0.0207 | Test Loss: 0.0218\n",
      "Epoch: 201/300 | Train Loss: 0.0207 | Test Loss: 0.0210\n",
      "Epoch: 202/300 | Train Loss: 0.0206 | Test Loss: 0.0214\n",
      "Epoch: 203/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 204/300 | Train Loss: 0.0205 | Test Loss: 0.0216\n",
      "Epoch: 205/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 206/300 | Train Loss: 0.0206 | Test Loss: 0.0214\n",
      "Epoch: 207/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 208/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 209/300 | Train Loss: 0.0206 | Test Loss: 0.0216\n",
      "Epoch: 210/300 | Train Loss: 0.0205 | Test Loss: 0.0213\n",
      "Epoch: 211/300 | Train Loss: 0.0207 | Test Loss: 0.0211\n",
      "Epoch: 212/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 213/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 214/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 215/300 | Train Loss: 0.0206 | Test Loss: 0.0213\n",
      "Epoch: 216/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 217/300 | Train Loss: 0.0206 | Test Loss: 0.0213\n",
      "Epoch: 218/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 219/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 220/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 221/300 | Train Loss: 0.0203 | Test Loss: 0.0211\n",
      "Epoch: 222/300 | Train Loss: 0.0203 | Test Loss: 0.0210\n",
      "Epoch: 223/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 224/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 225/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 226/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 227/300 | Train Loss: 0.0207 | Test Loss: 0.0213\n",
      "Epoch: 228/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 229/300 | Train Loss: 0.0205 | Test Loss: 0.0223\n",
      "Epoch: 230/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 231/300 | Train Loss: 0.0206 | Test Loss: 0.0215\n",
      "Epoch: 232/300 | Train Loss: 0.0205 | Test Loss: 0.0210\n",
      "Epoch: 233/300 | Train Loss: 0.0202 | Test Loss: 0.0212\n",
      "Epoch: 234/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 235/300 | Train Loss: 0.0204 | Test Loss: 0.0221\n",
      "Epoch: 236/300 | Train Loss: 0.0206 | Test Loss: 0.0223\n",
      "Epoch: 237/300 | Train Loss: 0.0207 | Test Loss: 0.0215\n",
      "Epoch: 238/300 | Train Loss: 0.0204 | Test Loss: 0.0216\n",
      "Epoch: 239/300 | Train Loss: 0.0203 | Test Loss: 0.0212\n",
      "Epoch: 240/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 241/300 | Train Loss: 0.0208 | Test Loss: 0.0213\n",
      "Epoch: 242/300 | Train Loss: 0.0203 | Test Loss: 0.0214\n",
      "Epoch: 243/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 244/300 | Train Loss: 0.0205 | Test Loss: 0.0211\n",
      "Epoch: 245/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 246/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 247/300 | Train Loss: 0.0204 | Test Loss: 0.0211\n",
      "Epoch: 248/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 249/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 250/300 | Train Loss: 0.0205 | Test Loss: 0.0223\n",
      "Epoch: 251/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 252/300 | Train Loss: 0.0202 | Test Loss: 0.0212\n",
      "Epoch: 253/300 | Train Loss: 0.0205 | Test Loss: 0.0219\n",
      "Epoch: 254/300 | Train Loss: 0.0204 | Test Loss: 0.0215\n",
      "Epoch: 255/300 | Train Loss: 0.0205 | Test Loss: 0.0216\n",
      "Epoch: 256/300 | Train Loss: 0.0205 | Test Loss: 0.0211\n",
      "Epoch: 257/300 | Train Loss: 0.0206 | Test Loss: 0.0210\n",
      "Epoch: 258/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 259/300 | Train Loss: 0.0203 | Test Loss: 0.0217\n",
      "Epoch: 260/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 261/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 262/300 | Train Loss: 0.0203 | Test Loss: 0.0213\n",
      "Epoch: 263/300 | Train Loss: 0.0206 | Test Loss: 0.0211\n",
      "Epoch: 264/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 265/300 | Train Loss: 0.0204 | Test Loss: 0.0213\n",
      "Epoch: 266/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 267/300 | Train Loss: 0.0204 | Test Loss: 0.0211\n",
      "Epoch: 268/300 | Train Loss: 0.0207 | Test Loss: 0.0209\n",
      "Epoch: 269/300 | Train Loss: 0.0208 | Test Loss: 0.0211\n",
      "Epoch: 270/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 271/300 | Train Loss: 0.0206 | Test Loss: 0.0210\n",
      "Epoch: 272/300 | Train Loss: 0.0206 | Test Loss: 0.0213\n",
      "Epoch: 273/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 274/300 | Train Loss: 0.0201 | Test Loss: 0.0209\n",
      "Epoch: 275/300 | Train Loss: 0.0203 | Test Loss: 0.0213\n",
      "Epoch: 276/300 | Train Loss: 0.0207 | Test Loss: 0.0212\n",
      "Epoch: 277/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 278/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 279/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 280/300 | Train Loss: 0.0205 | Test Loss: 0.0215\n",
      "Epoch: 281/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 282/300 | Train Loss: 0.0206 | Test Loss: 0.0212\n",
      "Epoch: 283/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 284/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 285/300 | Train Loss: 0.0204 | Test Loss: 0.0216\n",
      "Epoch: 286/300 | Train Loss: 0.0204 | Test Loss: 0.0211\n",
      "Epoch: 287/300 | Train Loss: 0.0206 | Test Loss: 0.0214\n",
      "Epoch: 288/300 | Train Loss: 0.0206 | Test Loss: 0.0215\n",
      "Epoch: 289/300 | Train Loss: 0.0204 | Test Loss: 0.0211\n",
      "Epoch: 290/300 | Train Loss: 0.0204 | Test Loss: 0.0221\n",
      "Epoch: 291/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 292/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 293/300 | Train Loss: 0.0204 | Test Loss: 0.0212\n",
      "Epoch: 294/300 | Train Loss: 0.0201 | Test Loss: 0.0212\n",
      "Epoch: 295/300 | Train Loss: 0.0204 | Test Loss: 0.0214\n",
      "Epoch: 296/300 | Train Loss: 0.0203 | Test Loss: 0.0213\n",
      "Epoch: 297/300 | Train Loss: 0.0205 | Test Loss: 0.0214\n",
      "Epoch: 298/300 | Train Loss: 0.0205 | Test Loss: 0.0212\n",
      "Epoch: 299/300 | Train Loss: 0.0204 | Test Loss: 0.0215\n",
      "Epoch: 300/300 | Train Loss: 0.0203 | Test Loss: 0.0211\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "batch_size = 2\n",
    "loss_fn = torch.nn.MSELoss() #check in WANDB\n",
    "activation_fn = torch.nn.ReLU()\n",
    "\n",
    "#Initialising the model, we might need to do 256x256 crops in the end\n",
    "model = UNet()\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "#Get the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train,y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test,y_test)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Optimiser\n",
    "#opt = Adam(model.parameters(), lr=learning_rate)\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss_per_epoch = 0\n",
    "    for i, (images, cellprobs) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        images = torch.unsqueeze(images,1)\n",
    "        outputs = model(images)\n",
    "        outputs = activation_fn(outputs)\n",
    "        loss = loss_fn(outputs, cellprobs)\n",
    "        total_train_loss_per_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    total_train_loss_per_epoch /= len(train_loader)\n",
    "    train_losses.append(total_train_loss_per_epoch)\n",
    "    \n",
    "    #get test loss\n",
    "    total_test_loss_per_epoch = 0\n",
    "    with torch.no_grad():\n",
    "        for images, cellprobs in test_loader:\n",
    "            images = torch.unsqueeze(images,1)\n",
    "            outputs = model(images)\n",
    "            outputs = activation_fn(outputs)\n",
    "            loss = loss_fn(outputs, cellprobs)\n",
    "            total_test_loss_per_epoch += loss.item()\n",
    "    total_test_loss_per_epoch /= len(test_loader)\n",
    "    test_losses.append(total_test_loss_per_epoch)\n",
    "\n",
    "    print('Epoch: {}/{} | Train Loss: {:.4f} | Test Loss: {:.4f}'.format(epoch+1, num_epochs, total_train_loss_per_epoch, total_test_loss_per_epoch))\n",
    "    #might be iunteresting to get a prediction at each epoch from the test loss and visualise it to track its progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(), 'cellprob_model_0.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
